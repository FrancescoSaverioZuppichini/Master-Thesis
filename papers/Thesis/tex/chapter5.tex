\documentclass[../document.tex]{subfiles}
\begin{document}
\chapter{Interpretability}
\label{chap: interpretability}
In this section, we will evaluate the model's prediction to better understand it. We will find if there are any features in the patches that can confuse it and if the model's output is robust.
First, we will show how the model learn to correctly separate the features from the two clases, then we will introduce on technique used to highlight the region of the input image that contribute the most to the model predictions. Then, we will use it on the data from the \emph{Quarry} test set to find out the patches were the model fails and analyze them.

Later, we will work with custom created patches with different features, walls, bumps, etc, to test the robustness of the model by comparing its predictions to the real data gathered from the simulator.


\subsection{Features separability}
In general, convolutional neural network learns to encode images by applying filters of increasing sizes at each layer. Usually, the first layers learn basic features, such us edges, while the final one encodes complex shapes. Those final informations are combined and mapped to the correct classes by one or more fully connected. For example, the following image was generated by plotting the learned features for different categories at different layers by 
Lee et al. \cite{deepbelief}
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{../img/5/deep_belief.png}
    \caption{Figure from Lee et al. \cite{deepbelief} paper where they shown for different classes the low-level features (up) and the high-level features (down) learned by a convolution neural network.}
\end{figure}
So, a correctly trained network should be able to separate those features based on predicted classes. Intuitivelly, given two classes $\mathcal{A}$ and $\mathcal{B}$, for example \emph{cat} and \emph{dog}, the high-level features for each class should not be the same, otherwise the model may output a wrong prediction since the same feature is maped to different classes. 
Thus, visualizing the inputs features vectors against each class can give a good quality estimation of the model since highly separable features yield a correct learning procedure.  Our model, MicroResNet, has a last convolution layer's features size of $[128, 3, 3 ]$. Since visualize a $128$ dimension vector is impossible, we reduce its dimension to $2$ by applying Principle Anyalisis Component (PCA) \cite{pca}. The following figures shows the features of $11$K images sampled from the train set label with their classes, \emph{traversable} and \emph{not traversable}.
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{1\textwidth}
        \includegraphics[width=\linewidth]{../img/5/pca/pca.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\linewidth]{../img/5/pca/pca-0.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\linewidth]{../img/5/pca/pca-1.png}
    \end{subfigure}
\caption{Principal Component Analysis}
\end{figure}
The two point clouds, left and right, are cleary separable meaning that the model was able to learn meaning features from the dataset. On the left there are the features of the not traversable images while on the right the ones for the traversable. One advatange of this procedure is that intrinsically, similar patches will be close to eachother helping to identify clusters of inputs. The following image shows the inputs based on their position in the features space.
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{1\textwidth}
        \includegraphics[width=\linewidth]{../img/5/pca/pca-patches-200.png}
    \end{subfigure}
    \begin{subfigure}[b]{1\textwidth}
        \includegraphics[width=\linewidth]{../img/5/pca/pca-train-patches-highlight.jpg}
    \end{subfigure}
\caption{TODO}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{1\textwidth}
        \includegraphics[width=\linewidth]{../img/5/pca/pca-test.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\linewidth]{../img/5/pca/pca-test-0.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\linewidth]{../img/5/pca/pca-test-1.png}
    \end{subfigure}
\caption{TODO}
\end{figure}

We can clearly see that patches with obstacles, bumps, bars and steps are correctly placed on the left side. Traversable inputs are on the right side, on the top part there are downhills. Goind down we can find little bumps and in the middle we have a big cloud of patches with rocks and small obstacles. Those patches are the most hard do classify since their features are the closest to the not traversable ones. 
 
% \input{./tex/chapter5-patches.tex}
% \input{./tex/chapter5-quarry.tex}

\section{Grad-CAM}
Gradient-weighted Class Activation Mapping (Grad-CAM) \cite{gradcam} is a technique to produce \"visual explanations\" for convolutional neural networks. It highlights the regions of the input image that contribuite the most to the prediciton. 
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{1\textwidth}
        \includegraphics[width=\linewidth]{../img/5/grad_cam1.png}
    \end{subfigure}
\caption{Grad-CAM procedure on an input image. Image from the original paper \cite{gradcam}.}
\end{figure}
In detail, the output with respect to a target class is backpropagate while storing the gradient and the output in the last convolution. Then, global average is applyed to the saved gradient keeping the channel dimension in order to get a 1-d tensor, this will represent the importance of each channel in the target convolutional layer. We  each element of the convolutional layer outputs is multiplyed with the averaged gradients to create the grad cam. This whole procedure is fast and it is architecture independent.

In detail, the output with respect to a target class is backpropagate while storing the gradient and the output in the last convolution. Then global average is applyed the saved gradient keeping the channel dimension in order to get a 1-d tensor, this will represent the importance of each channel in the target convolutional layer. We  each element of the convolutional layer outputs is multiplyed with the averaged gradients to create the grad cam. This whole procedure is fast and it is architecture independent.
\end{document}