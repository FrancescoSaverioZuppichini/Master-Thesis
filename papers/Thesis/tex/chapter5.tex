\documentclass[../document.tex]{subfiles}
\begin{document}
\todo[inline]{This chapter is under development!}
\chapter{Interpretability}
\label{chap: interpretability}
Understanding the model's strength, robustness and, limitations is crucial to gain a better understanding of its outputs. This is even more important when working with controllers that can be deployed in real scenarios. In this section, we evaluated the quality of our traversability estimator with different methodology. First, we showed that the model has correctly learned grounds' features and was able to separable terrains based on them. Second, we utilized the challenging Quarry dataset to visualize the most traversable, the lest traversable and the misclassified patches.  Utilizing a special method, we determined that the model always looked at the correct features in the ground even if when it fails. Lastly, we crafted patches with different unique features, such as walls, bumps, etc, to test the robustness of the model by comparing its outputs to the real data gathered from the simulator. 


\subsection{Features separability}
In general, convolutional neural networks learn to encode images by applying filters of increasing size at each layer. Usually, the first layers learn basic features, such as edges, while the final one encodes complex shapes. The final convolutional layer's outputs are usually referred to as features space, consequently, a feature vector is just the output of the last layer for a given image. Those last features are combined and mapped to the correct classes by one or more fully connected. The following image help to visualize the different features learned at each layer. It was generated by plotting the learned features for different categories at different layers by 
Lee et al. \cite{deepbelief}. 
\begin{figure} [htbp]
    \centering
    \includegraphics[width=\linewidth]{../img/5/deep_belief.png}
    \caption{Figure from Lee et al. \cite{deepbelief} paper where they showed for different classes the low-level features (up) and the high-level features (down) learned by a convolution neural network.}
\end{figure}
So, a correctly trained network should be able to separate those features based on predicted classes. Intuitively, given two classes $\mathcal{A}$ and $\mathcal{B}$, for example, \emph{cat} and \emph{dog}, the high-level features for each class should not be the same, otherwise, the model may misclassify the input due to the overlap of different classes' features. 
One technique to discover the degree of separability of our network is to directly visualize the inputs features vectors for each class. Unfortunately, like most models, our network, MicroResNet, has a high dimensional feature space. Each patch is mapped to a  $[128, 3, 3 ]$ vector. We cannot directly visualize a $128$ dimension space, we reduced each feature vector dimension to a two-dimension by applying Principle Analysis Component (PCA) \cite{pca} to visualize it. We investigate the features space of the model booth in the train and test set.

\subsection{Train set}
The following figures shows the features of $11$K images sampled from the train set labeled with their classes, \emph{traversable} and \emph{not traversable}. 
\begin{figure} [htbp]
    \centering
    \begin{subfigure}[b]{1\textwidth}
        \includegraphics[width=\linewidth]{../img/5/pca/pca.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\linewidth]{../img/5/pca/pca-0.png}
        \caption{Not Traversable}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\linewidth]{../img/5/pca/pca-1.png}
        \caption{Traversable}
    \end{subfigure}
\caption{Principal Component Analysis on the features space computed using the features from the last convolutional layers on the training dataset.}
\end{figure}
We can clearly recognize two main clusters based on the labels' color, one on the left and one of the right. Those points are easily separable, even by human eyes, meaning that the model was able to learn meaning features from the dataset.  To be sure the center of each class' point cloud is not overlapping we plotted the density of each cluster.
\begin{figure} [htbp]
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\linewidth]{../img/5/pca/pca-0-density.png}
        \caption{Not Traversable}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\linewidth]{../img/5/pca/pca-1-density.png}
        \caption{Traversable}
    \end{subfigure}
    \caption{Density plot for the points sampled from the training dataset in the features space. The centers of the cluster are not overlapping yielding a good separability and correct learning.}
    \end{figure}
Clearly, there is some distance between the centers. Furthermore, we can directly plot the patch corresponding to each feature vector to identify clusters of patches based on their position. Intuitively, if similar inputs are close to each other in the features space then the model also learned to effectively encode terrains features.   We decided to not show all images on the same plot to avoid overcrowding the image. Instead, we clustered the points using K-Means with $k=200$ clusters and then we took the patch that corresponded to the center point in each cluster. In this way, even if we are showing only a few inputs, we included all the meaningful features. The following image shows the result. 
\begin{figure} [htbp]
    \centering
    \begin{subfigure}[b]{1\textwidth}
        \includegraphics[width=\linewidth]{../img/5/pca/pca-patches-200.png}
    \end{subfigure}
    \caption{Patches plotted using the coordinate of the features vector obtained from the last convolutional layer's output and then reduced using PCA to a two-dimensional vector. Similar grounds are close to each other.}
\end{figure}
Definitely, patches with similar features are close to each other yielding a quality features encoding. On the left-top side, we can distinguish highly untraversable patches with walls/bumps in front of the robot. Going down, we encounter patches with smaller obstacles. On the plateau, there are traversable patches with small obstacles such as light bumps. Importantly,  those patches are the closest ones to the not traversable ones, so they were the hardest to separate, thus, to classify.  Going up on the right side, we see some grounds with small steps. Finally, on the top, we find all the downhill patches, the simplest ones to traverse.

\subsection{Test set}
We can apply the same procedure on the test set. Since it is a real world quarry, this dataset is harder than the train set and present challenging situations for the robot. The following image shows the features space after reducing its dimension to two using PCA.
\begin{figure} [htbp]
    \centering
    \begin{subfigure}[b]{1\textwidth}
        \includegraphics[width=\linewidth]{../img/5/pca/pca-test.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\linewidth]{../img/5/pca/pca-test-0.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\linewidth]{../img/5/pca/pca-test-1.png}
    \end{subfigure}
\caption{TODO}
\end{figure}
Interesting, the traversable patches are very near to each other, while the others span a very big surface. This suggests that there are many not traversable terrains with different features. The traversable points are clustered near the center, this implies that most of them share similar features. We plotted the density for each class to better understand where the most points are mapped.
\begin{figure} [htbp]
\begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\linewidth]{../img/5/pca/pca-test-0-density.png}
    \caption{Not Traversable}
\end{subfigure}
\begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\linewidth]{../img/5/pca/pca-test-1-density.png}
    \caption{Traversable}
    \label{fig : pca-test-density-1}
\end{subfigure}
\caption{Density plot for the points sampled from the test dataset in the features space. The centers of the cluster are not overlapping yielding a good separability and correct learning.}
\end{figure}
The two centers are really close to each other, making those samples harder to separate and some not traversable points are mixed up with the traversable ones. This explains the elevated number of false negative that lower down the AUC score on this dataset. We can also visualize the patches by plotting them using their features coordinates
\begin{figure} [htbp]
    \centering
    \begin{subfigure}[b]{1\textwidth}
        \includegraphics[width=\linewidth]{../img/5/pca/pca-test-patches-200-None-test.png}
    \end{subfigure}
\caption{Patches that correspond to coordinates in the features space of the last convolutional layers on the test dataset. Similar grounds are close to each other.}
\ref{fig: pca-test-patches}
\end{figure}
On the top left, from the not traversable cloud, we can see patches with a high level of bumps. Going down we find surfaces with huge walls in front of the robot while going close to the center we start to see all the traversable patches. Those samples have not too steep slopes. If we move to the density center, green double shown in figure \ref{pca-test-density-1}, we encounter lots of flat patches with little obstacles. Going up on the right branch we find downhill and on the top there are falls. 

In the following section, we will take a deep look at the test set to find which patches confuse the model. Most probably, those samples will be located between the two clusters center where the difference between classes' features is minimum.
\section{Grad-CAM}
Gradient-weighted Class Activation Mapping (Grad-CAM) \cite{gradcam} is a technique to produce \"visual explanations\" for convolutional neural networks. It highlights the regions of the input image that contribute the most to the predictions. 
\begin{figure} [htbp]
    \centering
    \begin{subfigure}[b]{1\textwidth}
        \includegraphics[width=\linewidth]{../img/5/grad_cam1.png}
    \end{subfigure}
\caption{Grad-CAM procedure on an input image. Image from the original paper \cite{gradcam}.}
\end{figure}
In detail, the output with respect to a target class is backpropagated while storing the gradient and the output in the last convolution. Then, a global average is applied to the saved gradient keeping the channel dimension in order to get a 1-d tensor, this will represent the importance of each channel in the target convolutional layer. After, each element of the convolutional layer outputs is multiplied with the averaged gradients to create the grad cam. This whole procedure is fast and it is architecture independent.

\input{./tex/chapter5-quarry.tex}
% \input{./tex/chapter5-patches.tex}
%

\end{document}