{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random((20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "from os import path \n",
    "import os\n",
    "from utilities.postprocessing.utils import KrockPatchExtractStrategy\n",
    "import os \n",
    "from utilities.postprocessing.handlers import *\n",
    "import zipfile\n",
    "import glob \n",
    "# import tqdm\n",
    "from tqdm import tqdm_notebook as bar\n",
    "import pypeln.thread as th\n",
    "from pydrive.auth import GoogleAuth\n",
    "\n",
    "drive_ids = {\n",
    "    'test': '1tN6jcMHiwLfDWogJ4YvbistPNlodikPI'\n",
    "}\n",
    "\n",
    "download_dir= '/home/francesco/Desktop/'\n",
    "\n",
    "class TraversabilityDataset(Dataset):\n",
    "    def __init__(self, df, images_dir, time_window):\n",
    "        self.df, self.images_dir, self.time_window = df, images_dir, time_window\n",
    "        \n",
    "        self.df = add_advancement(df, time_window)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df['images'][idx]\n",
    "        im = cv2.imread(self.images_dir + '/' + img_path)\n",
    "        return im\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.df)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_remote(cls, url, which='test', where=download_dir, *args, **kwargs):\n",
    "        zip_path = where + '/' + which + '.zip'\n",
    "        print('[INFO] Downloading using curl in {}...'.format(where), end='')\n",
    "#         old school never dies\n",
    "        os.system('curl -L -o {} \"https://drive.google.com/uc?export=download&id={}\"'.format(zip_path, drive_ids[which]))\n",
    "        print('done!')\n",
    "        return cls.from_zip(zip_path, out_dir=where, *args, **kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_zip(cls, zip_path, out_dir=download_dir, *args, **kwargs):\n",
    "        zip_ref = zipfile.ZipFile(zip_path, 'r')\n",
    "        extracted = zip_ref.namelist()\n",
    "        root = path.join(out_dir, extracted[0])\n",
    "        \n",
    "        should_unzip = not path.isdir(root)\n",
    "        if should_unzip:\n",
    "            print('[INFO] Extracting Zip file to {}...'.format(root), end='')\n",
    "            zip_ref.extractall(out_dir)\n",
    "            zip_ref.close()\n",
    "        else:\n",
    "            print('[INFO] Loading from {}...'.format(root), end='')\n",
    "        print('done!')\n",
    "        return cls.from_root(root=root, *args, **kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_meta(cls, meta, root, max_advancement, maps_dir, out_dir=None, time_window=None, *args, **kwargs):\n",
    "        ds = []\n",
    "        if out_dir is None: out_dir = root\n",
    "            \n",
    "        meta_out_dir = \"{}/csvs/\".format(out_dir)\n",
    "        patches_out_dir = \"{}/patches/{}/\".format(out_dir, max_advancement)\n",
    "        os.makedirs(meta_out_dir, exist_ok=True)\n",
    "        os.makedirs(patches_out_dir, exist_ok=True)\n",
    "\n",
    "        should_extract_patches = len(glob.glob(patches_out_dir + '/*.png')) <= 0\n",
    "        if should_extract_patches: \n",
    "            print('[INFO] extracting patches to {}...'.format(patches_out_dir), end='')\n",
    "            patches_extractor = MultiThreadWrapper(16, Compose([\n",
    "                ReadDataframeFilenameAndHm(meta_out_dir, maps_dir),\n",
    "                AddAdvancement(time_window),\n",
    "                ExtractPatches(patch_extract_stategy=KrockPatchExtractStrategy(max_advancement=max_advancement)),\n",
    "                StorePatches(patches_out_dir,meta_out_dir)\n",
    "            ]))\n",
    "\n",
    "            out = patches_extractor(meta.iterrows())\n",
    "            print('done!')\n",
    "        \n",
    "        print('[INFO] Creating datasets...', end='')\n",
    "        \n",
    "        pbar = bar(meta.iterrows())\n",
    "#         create all datasets       \n",
    "        for idx, row in pbar:       \n",
    "            filename, map = row['filename'], row['map']\n",
    "            pbar.set_description('[INFO] {}'.format(filename))\n",
    "            try:\n",
    "                df, hm = open_df_and_hm_from_meta_row(row, meta_out_dir, maps_dir)\n",
    "                ds.append(cls(df, patches_out_dir,time_window))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "                \n",
    "        print('done!')\n",
    "        return ConcatDataset(ds)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_root(cls, root, max_advancement, maps_dir=None, out_dir=None, *args, **kwargs):\n",
    "        maps_dir = root + '/maps/' if maps_dir is None else maps_dir\n",
    "        out_dir = root if out_dir is None else out_dir\n",
    "        \n",
    "        meta = pd.read_csv(root + '/meta.csv')\n",
    "        \n",
    "        return cls.from_meta(meta, root, max_advancement, maps_dir, out_dir, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating datasets..."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22c16d7c34e46e9a00555415bc58bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done!\n",
      "[[[ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  ...\n",
      "  [20 20 20]\n",
      "  [21 21 21]\n",
      "  [21 21 21]]\n",
      "\n",
      " [[ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  ...\n",
      "  [20 20 20]\n",
      "  [21 21 21]\n",
      "  [21 21 21]]\n",
      "\n",
      " [[ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  ...\n",
      "  [21 21 21]\n",
      "  [21 21 21]\n",
      "  [22 22 22]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 3  3  3]\n",
      "  [ 3  3  3]\n",
      "  [ 4  4  4]\n",
      "  ...\n",
      "  [34 34 34]\n",
      "  [35 35 35]\n",
      "  [35 35 35]]\n",
      "\n",
      " [[ 3  3  3]\n",
      "  [ 3  3  3]\n",
      "  [ 4  4  4]\n",
      "  ...\n",
      "  [32 32 32]\n",
      "  [34 34 34]\n",
      "  [34 34 34]]\n",
      "\n",
      " [[ 3  3  3]\n",
      "  [ 3  3  3]\n",
      "  [ 4  4  4]\n",
      "  ...\n",
      "  [33 33 33]\n",
      "  [33 33 33]\n",
      "  [34 34 34]]]\n"
     ]
    }
   ],
   "source": [
    "ds = TraversabilityDataset.from_root('/media/francesco/saetta/krock-dataset/val',  \n",
    "                                  max_advancement=0.66,\n",
    "                                     time_window=100)\n",
    "\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Downloading using curl in /home/francesco/Desktop/...done!\n",
      "[INFO] Extracting Zip file to /home/francesco/Desktop/slope/...done!\n",
      "[INFO] Creating datasets..."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9cd34b804be4a06a291746d6ea0556d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done!\n",
      "[[[ 72  72  72]\n",
      "  [ 73  73  73]\n",
      "  [ 73  73  73]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [115 115 115]\n",
      "  [116 116 116]]\n",
      "\n",
      " [[ 72  72  72]\n",
      "  [ 73  73  73]\n",
      "  [ 73  73  73]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [115 115 115]\n",
      "  [116 116 116]]\n",
      "\n",
      " [[ 72  72  72]\n",
      "  [ 73  73  73]\n",
      "  [ 73  73  73]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [115 115 115]\n",
      "  [116 116 116]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 75  75  75]\n",
      "  [ 76  76  76]\n",
      "  [ 76  76  76]\n",
      "  ...\n",
      "  [119 119 119]\n",
      "  [119 119 119]\n",
      "  [120 120 120]]\n",
      "\n",
      " [[ 75  75  75]\n",
      "  [ 76  76  76]\n",
      "  [ 76  76  76]\n",
      "  ...\n",
      "  [119 119 119]\n",
      "  [119 119 119]\n",
      "  [120 120 120]]\n",
      "\n",
      " [[ 75  75  75]\n",
      "  [ 76  76  76]\n",
      "  [ 76  76  76]\n",
      "  ...\n",
      "  [118 118 118]\n",
      "  [119 119 119]\n",
      "  [120 120 120]]]\n"
     ]
    }
   ],
   "source": [
    "ds = TraversabilityDataset.from_remote('asd',  \n",
    "                                  max_advancement=0.66,\n",
    "                                     maps_dir='./maps/new-train/',\n",
    "                                     time_window=100)\n",
    "\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ds = TraversabilityDataset.from_zip(\n",
    "                                    zip_path='/media/francesco/saetta/krock-dataset/val.zip',\n",
    "                                    out_dir='/media/francesco/saetta/zip/',\n",
    "                                     max_advancement=0.66,\n",
    "                                     maps_dir='./maps/val/',\n",
    "                                     time_window=100)\n",
    "\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv('/home/francesco/Desktop/test/meta.csv')\n",
    "ds = TraversabilityDataset.from_meta(meta,\n",
    "                                     root='/media/francesco/saetta/krock-dataset/train/',\n",
    "                                     out_dir='/home/francesco/Desktop/test/',\n",
    "                                     max_advancement=0.66,\n",
    "                                     maps_dir='./maps/train/',\n",
    "                                     time_window=100)\n",
    "\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/media/francesco/saetta/krock-dataset/train/csvs/slope_rocks1-7.0-26.csv')\n",
    "meta = pd.read_csv('/home/francesco/Desktop/test/meta.csv')\n",
    "\n",
    "hm = cv2.imread('./maps/new-train/slope_rocks1.png')\n",
    "hm = cv2.cvtColor(hm, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "ds = TraversabilityDataset.from_df(df, hm, 0.66,\n",
    "                                               out_dir='/home/francesco/Desktop/test/')\n",
    "\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.postprocessing.handlers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61069966, 0.75589338, 0.18784141, 0.9053972 , 0.40566203,\n",
       "       0.59764261, 0.04879094, 0.40811138, 0.48616502, 0.25838815,\n",
       "       0.24416049, 0.4908535 , 0.90192317, 0.05312703, 0.75706453,\n",
       "       0.60262945, 0.1131344 , 0.25877077, 0.85337593, 0.27058206])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.random((20))\n",
    "b = np.random.random((80))\n",
    "\n",
    "tot = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
