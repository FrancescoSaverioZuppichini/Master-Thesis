{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random((20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "from os import path \n",
    "import os\n",
    "from utilities.pipeline import *\n",
    "from utilities.postprocessing.utils import KrockPatchExtractStrategy\n",
    "import os \n",
    "from utilities.postprocessing.handlers import *\n",
    "import zipfile\n",
    "import glob \n",
    "# import tqdm\n",
    "from tqdm import tqdm_notebook as bar\n",
    "import pypeln.thread as th\n",
    "from pydrive.auth import GoogleAuth\n",
    "\n",
    "drive_ids = {\n",
    "    'test': '1tN6jcMHiwLfDWogJ4YvbistPNlodikPI'\n",
    "}\n",
    "\n",
    "download_dir= '/home/francesco/Desktop/'\n",
    "\n",
    "class TraversabilityDataset(Dataset):\n",
    "    def __init__(self, df, images_dir, time_window):\n",
    "        self.df, self.images_dir, self.time_window = df, images_dir, time_window\n",
    "        \n",
    "        self.df = add_advancement(df, time_window)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df['images'][idx]\n",
    "        im = cv2.imread(self.images_dir + '/' + img_path)\n",
    "        return im\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.df)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_remote(cls, url, which='test', where=download_dir, *args, **kwargs):\n",
    "        zip_path = where + '/' + which + '.zip'\n",
    "        print('[INFO] Downloading using curl in {}...'.format(where), end='')\n",
    "#         old school never dies\n",
    "        os.system('curl -L -o {} \"https://drive.google.com/uc?export=download&id={}\"'.format(zip_path, drive_ids[which]))\n",
    "        print('done!')\n",
    "        return cls.from_zip(zip_path, out_dir=where, *args, **kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_zip(cls, zip_path, out_dir=download_dir, *args, **kwargs):\n",
    "        zip_ref = zipfile.ZipFile(zip_path, 'r')\n",
    "        extracted = zip_ref.namelist()\n",
    "        root = path.join(out_dir, extracted[0])\n",
    "        \n",
    "        should_unzip = not path.isdir(root)\n",
    "        if should_unzip:\n",
    "            print('[INFO] Extracting Zip file to {}...'.format(root), end='')\n",
    "            zip_ref.extractall(out_dir)\n",
    "            zip_ref.close()\n",
    "        else:\n",
    "            print('[INFO] Loading from {}...'.format(root), end='')\n",
    "        print('done!')\n",
    "        return cls.from_root(root=root, *args, **kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_meta(cls, meta, root, max_advancement, maps_dir, out_dir=None, time_window=None, *args, **kwargs):\n",
    "        ds = []\n",
    "        if out_dir is None: out_dir = root\n",
    "            \n",
    "        meta_out_dir = \"{}/csvs/\".format(out_dir)\n",
    "        patches_out_dir = \"{}/patches/{}/\".format(out_dir, max_advancement)\n",
    "        os.makedirs(meta_out_dir, exist_ok=True)\n",
    "        os.makedirs(patches_out_dir, exist_ok=True)\n",
    "\n",
    "        should_extract_patches = len(glob.glob(patches_out_dir + '/*.png')) <= 0\n",
    "        if should_extract_patches: \n",
    "            print('[INFO] extracting patches to {}...'.format(patches_out_dir), end='')\n",
    "            patches_extractor = MultiThreadWrapper(16, Compose([\n",
    "                ReadDataframeFilenameAndHm(meta_out_dir, maps_dir),\n",
    "#                 AddAdvancement(time_window),\n",
    "                ExtractPatches(patch_extract_stategy=KrockPatchExtractStrategy(max_advancement=max_advancement)),\n",
    "                StorePatches(patches_out_dir,meta_out_dir)\n",
    "            ]))\n",
    "\n",
    "            out = patches_extractor(meta.iterrows())\n",
    "            print('done!')\n",
    "        \n",
    "        print('[INFO] Creating datasets...', end='')\n",
    "        \n",
    "        pbar = bar(meta.iterrows())\n",
    "#         create all datasets       \n",
    "        for idx, row in pbar:       \n",
    "            filename, map = row['filename'], row['map']\n",
    "            pbar.set_description('[INFO] {}'.format(filename))\n",
    "            try:\n",
    "                df, hm = open_df_and_hm_from_meta_row(row, meta_out_dir, maps_dir)\n",
    "                ds.append(cls(df, patches_out_dir,time_window))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "                \n",
    "        print('done!')\n",
    "        return ConcatDataset(ds)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_root(cls, root, max_advancement, maps_dir=None, out_dir=None, *args, **kwargs):\n",
    "        maps_dir = root + '/maps/' if maps_dir is None else maps_dir\n",
    "        out_dir = root if out_dir is None else out_dir\n",
    "        \n",
    "        meta = pd.read_csv(root + '/meta.csv')\n",
    "        \n",
    "        return cls.from_meta(meta, root, max_advancement, maps_dir, out_dir, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] extracting patches to /media/francesco/saetta/krock-dataset/test/patches/0.66/..."
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "\n\nOriginal Traceback (most recent call last):\n  File \"/home/francesco/.local/lib/python3.6/site-packages/pypeln/thread.py\", line 164, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/francesco/.local/lib/python3.6/site-packages/pypeln/thread.py\", line 214, in f_task\n    y = f(x, *args)\n  File \"/home/francesco/Documents/Master-Thesis/core/utilities/pipeline/pipeline.py\", line 15, in __call__\n    res = self.handlers[0](*args, **kwargs)\n  File \"/home/francesco/Documents/Master-Thesis/core/utilities/postprocessing/handlers/handlers.py\", line 33, in __call__\n    df, hm = open_df_and_hm_from_meta_row(row, self.base_dir, self.hm_dir)\n  File \"/home/francesco/Documents/Master-Thesis/core/utilities/postprocessing/handlers/functional.py\", line 171, in open_df_and_hm_from_meta_row\n    df = pd.read_csv(base_dir + '/' + filename + '.csv')\n  File \"/home/francesco/.local/lib/python3.6/site-packages/pandas/io/parsers.py\", line 697, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/francesco/.local/lib/python3.6/site-packages/pandas/io/parsers.py\", line 424, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/francesco/.local/lib/python3.6/site-packages/pandas/io/parsers.py\", line 890, in __init__\n    self._make_engine(self.engine)\n  File \"/home/francesco/.local/lib/python3.6/site-packages/pandas/io/parsers.py\", line 1117, in _make_engine\n    self._engine = CParserWrapper(self.f, **self.options)\n  File \"/home/francesco/.local/lib/python3.6/site-packages/pandas/io/parsers.py\", line 1848, in __init__\n    self._reader = parsers.TextReader(src, **kwds)\n  File \"pandas/_libs/parsers.pyx\", line 387, in pandas._libs.parsers.TextReader.__cinit__\n  File \"pandas/_libs/parsers.pyx\", line 705, in pandas._libs.parsers.TextReader._setup_parser_source\nFileNotFoundError: [Errno 2] File b'/media/francesco/saetta/krock-dataset/test/csvs//querry-big-10-10.0-1.csv' does not exist: b'/media/francesco/saetta/krock-dataset/test/csvs//querry-big-10-10.0-1.csv'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4dd983704f7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m ds = TraversabilityDataset.from_root('/media/francesco/saetta/krock-dataset/test',  \n\u001b[1;32m      2\u001b[0m                                   \u001b[0mmax_advancement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.66\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                time_window=100)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-069e4bf33cbf>\u001b[0m in \u001b[0;36mfrom_root\u001b[0;34m(cls, root, max_advancement, maps_dir, out_dir, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/meta.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_advancement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaps_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-069e4bf33cbf>\u001b[0m in \u001b[0;36mfrom_meta\u001b[0;34m(cls, meta, root, max_advancement, maps_dir, out_dir, time_window, *args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m             ]))\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatches_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Master-Thesis/core/utilities/pipeline/pipeline.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \"\"\"), fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pypeln/thread.py\u001b[0m in \u001b[0;36m_to_iterable\u001b[0;34m(stage, maxsize)\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpipeline_namespace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0merror_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_error_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\nOriginal {trace}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: \n\nOriginal Traceback (most recent call last):\n  File \"/home/francesco/.local/lib/python3.6/site-packages/pypeln/thread.py\", line 164, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/francesco/.local/lib/python3.6/site-packages/pypeln/thread.py\", line 214, in f_task\n    y = f(x, *args)\n  File \"/home/francesco/Documents/Master-Thesis/core/utilities/pipeline/pipeline.py\", line 15, in __call__\n    res = self.handlers[0](*args, **kwargs)\n  File \"/home/francesco/Documents/Master-Thesis/core/utilities/postprocessing/handlers/handlers.py\", line 33, in __call__\n    df, hm = open_df_and_hm_from_meta_row(row, self.base_dir, self.hm_dir)\n  File \"/home/francesco/Documents/Master-Thesis/core/utilities/postprocessing/handlers/functional.py\", line 171, in open_df_and_hm_from_meta_row\n    df = pd.read_csv(base_dir + '/' + filename + '.csv')\n  File \"/home/francesco/.local/lib/python3.6/site-packages/pandas/io/parsers.py\", line 697, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/francesco/.local/lib/python3.6/site-packages/pandas/io/parsers.py\", line 424, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/francesco/.local/lib/python3.6/site-packages/pandas/io/parsers.py\", line 890, in __init__\n    self._make_engine(self.engine)\n  File \"/home/francesco/.local/lib/python3.6/site-packages/pandas/io/parsers.py\", line 1117, in _make_engine\n    self._engine = CParserWrapper(self.f, **self.options)\n  File \"/home/francesco/.local/lib/python3.6/site-packages/pandas/io/parsers.py\", line 1848, in __init__\n    self._reader = parsers.TextReader(src, **kwds)\n  File \"pandas/_libs/parsers.pyx\", line 387, in pandas._libs.parsers.TextReader.__cinit__\n  File \"pandas/_libs/parsers.pyx\", line 705, in pandas._libs.parsers.TextReader._setup_parser_source\nFileNotFoundError: [Errno 2] File b'/media/francesco/saetta/krock-dataset/test/csvs//querry-big-10-10.0-1.csv' does not exist: b'/media/francesco/saetta/krock-dataset/test/csvs//querry-big-10-10.0-1.csv'\n"
     ]
    }
   ],
   "source": [
    "ds = TraversabilityDataset.from_root('/media/francesco/saetta/krock-dataset/test',  \n",
    "                                  max_advancement=0.71,\n",
    "                               time_window=100)\n",
    "\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Downloading using curl in /home/francesco/Desktop/...done!\n",
      "[INFO] Extracting Zip file to /home/francesco/Desktop/slope/...done!\n",
      "[INFO] Creating datasets..."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9cd34b804be4a06a291746d6ea0556d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done!\n",
      "[[[ 72  72  72]\n",
      "  [ 73  73  73]\n",
      "  [ 73  73  73]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [115 115 115]\n",
      "  [116 116 116]]\n",
      "\n",
      " [[ 72  72  72]\n",
      "  [ 73  73  73]\n",
      "  [ 73  73  73]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [115 115 115]\n",
      "  [116 116 116]]\n",
      "\n",
      " [[ 72  72  72]\n",
      "  [ 73  73  73]\n",
      "  [ 73  73  73]\n",
      "  ...\n",
      "  [114 114 114]\n",
      "  [115 115 115]\n",
      "  [116 116 116]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 75  75  75]\n",
      "  [ 76  76  76]\n",
      "  [ 76  76  76]\n",
      "  ...\n",
      "  [119 119 119]\n",
      "  [119 119 119]\n",
      "  [120 120 120]]\n",
      "\n",
      " [[ 75  75  75]\n",
      "  [ 76  76  76]\n",
      "  [ 76  76  76]\n",
      "  ...\n",
      "  [119 119 119]\n",
      "  [119 119 119]\n",
      "  [120 120 120]]\n",
      "\n",
      " [[ 75  75  75]\n",
      "  [ 76  76  76]\n",
      "  [ 76  76  76]\n",
      "  ...\n",
      "  [118 118 118]\n",
      "  [119 119 119]\n",
      "  [120 120 120]]]\n"
     ]
    }
   ],
   "source": [
    "ds = TraversabilityDataset.from_remote('asd',  \n",
    "                                  max_advancement=0.71,\n",
    "                                     maps_dir='./maps/new-train/',\n",
    "                                     time_window=100)\n",
    "\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ds = TraversabilityDataset.from_zip(\n",
    "                                    zip_path='/media/francesco/saetta/krock-dataset/val.zip',\n",
    "                                    out_dir='/media/francesco/saetta/zip/',\n",
    "                                     max_advancement=0.66,\n",
    "                                     maps_dir='./maps/val/',\n",
    "                                     time_window=100)\n",
    "\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv('/home/francesco/Desktop/test/meta.csv')\n",
    "ds = TraversabilityDataset.from_meta(meta,\n",
    "                                     root='/media/francesco/saetta/krock-dataset/train/',\n",
    "                                     out_dir='/home/francesco/Desktop/test/',\n",
    "                                     max_advancement=0.66,\n",
    "                                     maps_dir='./maps/train/',\n",
    "                                     time_window=100)\n",
    "\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/media/francesco/saetta/krock-dataset/train/csvs/slope_rocks1-7.0-26.csv')\n",
    "meta = pd.read_csv('/home/francesco/Desktop/test/meta.csv')\n",
    "\n",
    "hm = cv2.imread('./maps/new-train/slope_rocks1.png')\n",
    "hm = cv2.cvtColor(hm, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "ds = TraversabilityDataset.from_df(df, hm, 0.66,\n",
    "                                               out_dir='/home/francesco/Desktop/test/')\n",
    "\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.postprocessing.handlers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61069966, 0.75589338, 0.18784141, 0.9053972 , 0.40566203,\n",
       "       0.59764261, 0.04879094, 0.40811138, 0.48616502, 0.25838815,\n",
       "       0.24416049, 0.4908535 , 0.90192317, 0.05312703, 0.75706453,\n",
       "       0.60262945, 0.1131344 , 0.25877077, 0.85337593, 0.27058206])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.random((20))\n",
    "b = np.random.random((80))\n",
    "\n",
    "tot = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
